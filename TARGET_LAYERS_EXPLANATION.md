# hparams.target_layers å®é™…å€¼è¯´æ˜

## ğŸ“‹ target_layers çš„æ¥æº

åœ¨ TokenEdit ä¸­ï¼Œ`target_layers` æœ‰ä¸¤ä¸ªå¯èƒ½çš„æ¥æºï¼š

### 1. ä» `model_config.py` ä¼ å…¥ï¼ˆæ¨èï¼‰

åœ¨ [test_tokenedit_quick.py](test_tokenedit_quick.py) ä¸­ï¼š

```python
# åŠ è½½æ¨¡å‹é…ç½®
model, tokenizer, config = load_model_optimized(model_name)

# ä¼ å…¥ target_layers
hparams = TokenEditHyperParams(
    model_name=model_name,
    target_layers=config['target_layers'],  # â† è¿™é‡Œä¼ å…¥
    ...
)
```

### 2. è‡ªåŠ¨è®¾ç½®ï¼ˆæœªä¼ å…¥æ—¶ï¼‰

å¦‚æœ `target_layers=None`ï¼Œ[TokenEditEditor.__init__](tokenedit/tokenedit_main.py:46-49) ä¼šè°ƒç”¨ï¼š

```python
if hparams.target_layers is None:
    hparams.target_layers = self._get_default_target_layers(model)
```

---

## ğŸ”¢ å„æ¨¡å‹çš„å®é™…å€¼

### GPT-2-XL (1.5B å‚æ•°, 48 å±‚)

```python
# æ¥è‡ª model_config.py
target_layers = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
```

**è¯´æ˜ï¼š** ä¸­é—´ 1/3 çš„å±‚ï¼ˆå…± 10 å±‚ï¼‰
- æ€»å±‚æ•°ï¼š48
- é€‰æ‹©èŒƒå›´ï¼šå±‚ 15-24
- ç­–ç•¥ï¼šé¿å¼€æœ€æµ…å’Œæœ€æ·±çš„å±‚ï¼Œé€‰æ‹©è¯­ä¹‰è¡¨ç¤ºçš„ä¸­å±‚

---

### GPT-J-6B (6B å‚æ•°, 28 å±‚)

```python
# æ¥è‡ª model_config.py
target_layers = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
```

**è¯´æ˜ï¼š** ä¸­é—´å±‚ï¼ˆå…± 10 å±‚ï¼‰
- æ€»å±‚æ•°ï¼š28
- é€‰æ‹©èŒƒå›´ï¼šå±‚ 9-18
- ç­–ç•¥ï¼šé€‰æ‹©æ¨¡å‹ä¸­éƒ¨çš„å±‚

---

### LLaMA-3-8B (8B å‚æ•°, 32 å±‚)

```python
# æ¥è‡ª model_config.py
target_layers = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
```

**è¯´æ˜ï¼š** ä¸­é—´ååå±‚ï¼ˆå…± 12 å±‚ï¼‰
- æ€»å±‚æ•°ï¼š32
- é€‰æ‹©èŒƒå›´ï¼šå±‚ 10-21
- ç­–ç•¥ï¼šé€‰æ‹©ä¸­åéƒ¨çš„å±‚ï¼Œè¿™äº›å±‚é€šå¸¸åŒ…å«æ›´å¤šçŸ¥è¯†

---

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å±‚ï¼Ÿ

### ç†è®ºä¾æ®

1. **æµ…å±‚ï¼ˆå‰ 1/3ï¼‰**
   - ä¸»è¦å¤„ç†è¯­æ³•å’Œä½çº§ç‰¹å¾
   - çŸ¥è¯†è¡¨ç¤ºè¾ƒå°‘

2. **ä¸­å±‚ï¼ˆä¸­é—´ 1/3ï¼‰** â† **Target Layers**
   - åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤º
   - çŸ¥è¯†å­˜å‚¨çš„å…³é”®åŒºåŸŸ
   - **æœ€é€‚åˆè¿›è¡ŒçŸ¥è¯†ç¼–è¾‘**

3. **æ·±å±‚ï¼ˆå 1/3ï¼‰**
   - é«˜çº§æŠ½è±¡æ¨ç†
   - å¯èƒ½è¿‡åº¦æ‹Ÿåˆç‰¹å®šä»»åŠ¡
   - ä¿®æ”¹å¯èƒ½å½±å“æ¨¡å‹é€šç”¨æ€§

### TokenEdit çš„é€‰æ‹©ç­–ç•¥

TokenEdit è®ºæ–‡å»ºè®®ï¼š
- GPT-2: ä½¿ç”¨ä¸­é—´ 1/3 å±‚
- GPT-J: ä½¿ç”¨ä¸­é—´å±‚
- LLaMA: ä½¿ç”¨ä¸­åéƒ¨å±‚

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| æ¨¡å‹ | æ€»å±‚æ•° | Target Layers | å±‚æ•°å æ¯” | å±‚èŒƒå›´ |
|------|--------|---------------|----------|--------|
| **GPT-2-XL** | 48 | 10 å±‚ | 20.8% | 15-24 |
| **GPT-J-6B** | 28 | 10 å±‚ | 35.7% | 9-18 |
| **LLaMA-3-8B** | 32 | 12 å±‚ | 37.5% | 10-21 |

---

## ğŸ”§ å¦‚ä½•ä¿®æ”¹ target_layersï¼Ÿ

### æ–¹æ³• 1: ä¿®æ”¹ model_config.py

```python
MODEL_CONFIGS = {
    "gpt2-xl": {
        ...
        "target_layers": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],  # ä¿®æ”¹è¿™é‡Œ
        ...
    },
}
```

### æ–¹æ³• 2: ç›´æ¥ä¼ å…¥ hparams

```python
hparams = TokenEditHyperParams(
    model_name="gpt2-xl",
    target_layers=[20, 21, 22, 23, 24, 25, 26, 27, 28, 29],  # è‡ªå®šä¹‰
    ...
)
```

### æ–¹æ³• 3: è®©ä»£ç è‡ªåŠ¨è®¾ç½®

```python
hparams = TokenEditHyperParams(
    model_name="gpt2-xl",
    target_layers=None,  # ä½¿ç”¨é»˜è®¤å€¼
    ...
)
```

---

## âš™ï¸ è‡ªåŠ¨è®¾ç½®çš„é»˜è®¤å€¼

å¦‚æœ `target_layers=None`ï¼Œ[tokenedit_main.py::_get_default_target_layers](tokenedit/tokenedit_main.py:67-96) ä¼šè‡ªåŠ¨è®¾ç½®ï¼š

```python
def _get_default_target_layers(self, model) -> List[int]:
    """æ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ç›®æ ‡å±‚"""
    model_name = model.config._name_or_path.lower()

    if 'gpt2' in model_name:
        if 'xl' in model_name:
            return [17, 18, 19]  # GPT-2-XL: 3å±‚
        elif 'large' in model_name:
            return [14, 15, 16]  # GPT2-Large: 3å±‚
        elif 'medium' in model_name:
            return [9, 10, 11]   # GPT2-Medium: 3å±‚
        else:
            return [5, 6, 7]     # GPT2-Small: 3å±‚
    elif 'llama' in model_name:
        num_layers = model.config.num_hidden_layers
        return list(range(max(0, num_layers - 3), num_layers))  # æœ€å3å±‚
    else:
        num_layers = model.config.num_hidden_layers
        return list(range(max(0, num_layers - 3), num_layers))  # æœ€å3å±‚
```

**æ³¨æ„ï¼š** è‡ªåŠ¨è®¾ç½®åªé€‰æ‹© 3 å±‚ï¼Œæ¯” model_config.py ä¸­çš„é…ç½®å°‘ã€‚

---

## ğŸ’¡ å»ºè®®

1. **ä½¿ç”¨ model_config.py çš„é…ç½®**ï¼ˆæ¨èï¼‰
   - ç»è¿‡ä¼˜åŒ–çš„å±‚æ•°å’ŒèŒƒå›´
   - æ›´å¥½çš„ç¼–è¾‘æ•ˆæœ

2. **è°ƒè¯•æ—¶å¯ä»¥å‡å°‘å±‚æ•°**
   - é€‰æ‹© 3-5 å±‚å¯ä»¥åŠ å¿«è®­ç»ƒ
   - ä½†å¯èƒ½å½±å“ç¼–è¾‘æ•ˆæœ

3. **ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å®Œæ•´é…ç½®**
   - ä½¿ç”¨ 10-12 å±‚
   - æ›´ç¨³å®šçš„ç¼–è¾‘æ•ˆæœ

---

## ğŸ“ å¿«é€ŸæŸ¥çœ‹å½“å‰å€¼

åœ¨æµ‹è¯•è„šæœ¬ä¸­æ·»åŠ ï¼š

```python
print(f"\nå½“å‰ target_layers é…ç½®:")
print(f"  æ¨¡å‹: {hparams.model_name}")
print(f"  ç›®æ ‡å±‚: {hparams.target_layers}")
print(f"  å±‚æ•°: {len(hparams.target_layers)} å±‚")
print(f"  èŒƒå›´: {min(hparams.target_layers)}-{max(hparams.target_layers)}")
```

---

## ğŸ” éªŒè¯

è¿è¡Œå¿«é€Ÿæµ‹è¯•æ—¶ï¼Œæ‚¨ä¼šçœ‹åˆ°ï¼š

```
[2/4] é…ç½®å‚æ•°...
  è®­ç»ƒè½®æ•°: 20
  ç›®æ ‡å±‚: [15, 16, 17]...[22, 23, 24]
```

è¿™è¡¨ç¤ºï¼š
- ä½¿ç”¨ GPT-2-XL æ¨¡å‹
- ç›®æ ‡å±‚ä» 15 åˆ° 24
- å…± 10 å±‚

---

**æ€»ç»“ï¼š** å¯¹äº gpt2-xlï¼Œ`hparams.target_layers` çš„å®é™…å€¼æ˜¯ **[15, 16, 17, 18, 19, 20, 21, 22, 23, 24]**ï¼Œå…± 10 å±‚ã€‚
