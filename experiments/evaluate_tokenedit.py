# -*- coding: utf-8 -*-
"""è¯„ä¼°TokenEditæ–¹æ³• - æ”¯æŒå¤šä¸ªæ¨¡å‹"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)

import torch
import json
from pathlib import Path
from tqdm import tqdm
from tokenedit import TokenEditEditor, TokenEditHyperParams

try:
    from model_config import load_model_optimized, get_model_config
except ImportError as e:
    print(f"é”™è¯¯: æ— æ³•å¯¼å…¥ model_config")
    print(f"Pythonè·¯å¾„: {sys.path}")
    print(f"é¡¹ç›®æ ¹ç›®å½? {project_root}")
    print(f"è¯·ç¡®ä¿?model_config.py åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸?)
    sys.exit(1)

def load_hparams_from_json(model_name: str, hparams_dir: str = "hparams/TokenEdit"):
    """
    ä»JSONæ–‡ä»¶åŠ è½½è¶…å‚æ•°é…ç½?

    Args:
        model_name: æ¨¡å‹åç§°
        hparams_dir: è¶…å‚æ•°é…ç½®ç›®å½?

    Returns:
        TokenEditHyperParamså¯¹è±¡
    """
    hparams_path = Path(hparams_dir) / f"{model_name}.json"

    if not hparams_path.exists():
        print(f"âš?è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»?{hparams_path}")
        print(f"å°†ä½¿ç”¨é»˜è®¤è¶…å‚æ•°")
        return TokenEditHyperParams(model_name=model_name)

    print(f"âœ?ä»?{hparams_path} åŠ è½½é…ç½®")

    with open(hparams_path, 'r') as f:
        config = json.load(f)

    # æ‰“å°å…³é”®é…ç½®
    print(f"  é…ç½®å‚æ•°:")
    print(f"    - target_layers: {config.get('target_layers', 'æœªè®¾ç½?)}")
    print(f"    - num_epochs: {config.get('num_epochs', 100)}")
    print(f"    - learning_rate: {config.get('learning_rate', 0.001)}")
    print(f"    - w_edit: {config.get('w_edit', 1.0)}")
    print(f"    - w_suppress: {config.get('w_suppress', 0.5)}")

    # åˆ›å»ºTokenEditHyperParamså¯¹è±¡
    return TokenEditHyperParams(**config)

def load_data(num_samples=10):
    """åŠ è½½æ•°æ®"""
    data_path = Path("data/sample_data.json")
    if not data_path.exists():
        data_path = Path("data/counterfact.json")
    
    with open(data_path) as f:
        data = json.load(f)
    
    requests = []
    for item in data[:num_samples]:
        req = item['requested_rewrite']
        requests.append({
            'prompt': req['prompt'].format(req['subject']),
            'subject': req['subject'],
            'relation': 'capital',
            'target_new': req['target_new']['str'],
            'target_true': req['target_true']['str'],
            'paraphrase_prompts': item.get('paraphrase_prompts', []),
            'neighborhood_prompts': item.get('neighborhood_prompts', [])
        })
    return requests

def evaluate(editor, requests):
    """è¯„ä¼°æŒ‡æ ‡"""
    print("\nè¯„ä¼°ä¸?..")
    
    # Efficacy
    correct = 0
    for req in tqdm(requests, desc="Efficacy"):
        try:
            output = editor.inference(req['prompt'], max_new_tokens=5)
            if req['target_new'].lower() in output.lower():
                correct += 1
        except Exception as e:
            print(f"æ¨ç†é”™è¯¯: {e}")
    
    efficacy = correct / len(requests)
    print(f"âœ?ç¼–è¾‘æˆåŠŸç? {efficacy:.2%}")
    
    # Paraphrase
    para_correct = 0
    para_total = 0
    for req in tqdm(requests[:5], desc="Paraphrase"):  # åªæµ‹è¯•å‰5ä¸ªä»¥èŠ‚çœæ—¶é—´
        for para in req.get('paraphrase_prompts', [])[:2]:
            try:
                output = editor.inference(para, max_new_tokens=5)
                if req['target_new'].lower() in output.lower():
                    para_correct += 1
                para_total += 1
            except Exception as e:
                print(f"æ¨ç†é”™è¯¯: {e}")
    
    paraphrase = para_correct / para_total if para_total > 0 else 0.0
    print(f"âœ?æ³›åŒ–èƒ½åŠ›: {paraphrase:.2%}")
    
    return {
        'efficacy': efficacy,
        'paraphrase': paraphrase
    }

def main(model_name="gpt2-xl", num_samples=10, num_epochs=None):
    """
    ä¸»è¯„ä¼°å‡½æ•?
    
    Args:
        model_name: æ¨¡å‹åç§°
        num_samples: ç¼–è¾‘æ ·æœ¬æ•?
        num_epochs: è®­ç»ƒè½®æ•°
    """
    print("="*70)
    print(f"TokenEdit è¯„ä¼°å®éªŒ - {model_name}")
    print("="*70)
    
    # åŠ è½½æ¨¡å‹
    print("\n[1/4] åŠ è½½æ¨¡å‹...")
    model, tokenizer, _ = load_model_optimized(model_name)
    
    # åŠ è½½æ•°æ®
    print("\n[2/4] åŠ è½½æ•°æ®...")
    requests = load_data(num_samples)
    print(f"âœ?å·²åŠ è½?{len(requests)} ä¸ªç¼–è¾‘æ ·æœ?)
    
    # åˆ›å»ºç¼–è¾‘å™?
    print("\n[3/4] åˆ›å»ºç¼–è¾‘å™?..")

    # ä»JSONæ–‡ä»¶åŠ è½½è¶…å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€?
    hparams = load_hparams_from_json(model_name)

    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†num_epochsï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€?
    if num_epochs is not None:
        original_epochs = hparams.num_epochs
        hparams.num_epochs = num_epochs
        print(f"  è¦†ç›– num_epochs: {original_epochs} -> {num_epochs}")
    else:
        print(f"  ä½¿ç”¨é…ç½®æ–‡ä»¶çš?num_epochs: {hparams.num_epochs}")

    # ç¡®ä¿deviceè®¾ç½®æ­£ç¡®
    hparams.device = "cuda" if torch.cuda.is_available() else "cpu"
    hparams.verbose = True  # è¯„ä¼°æ—¶å‡å°‘è¾“å‡?

    editor = TokenEditEditor(model, tokenizer, hparams)
    
    # åº”ç”¨ç¼–è¾‘
    print("\n[4/4] åº”ç”¨ç¼–è¾‘...")
    try:
        editor.apply_edits(requests)
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("\nâ?æ˜¾å­˜ä¸è¶³ï¼å°è¯•å‡å°‘æ ·æœ¬æ•°æˆ–è½®æ•?)
            return
        raise
    
    # è¯„ä¼°
    metrics = evaluate(editor, requests)
    
    # ä¿å­˜ç»“æœ
    results = {
        'method': 'TokenEdit',
        'model': model_name,
        'num_samples': num_samples,
        'num_epochs': hparams.num_epochs,  # ä½¿ç”¨å®é™…çš„è®­ç»ƒè½®æ•?
        'metrics': metrics
    }
    
    Path("results").mkdir(exist_ok=True)
    results_file = f"results/tokenedit_{model_name.replace('/', '_')}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ?è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ? {results_file}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='gpt2-xl',
                       choices=['gpt2-xl', 'gpt-j-6b', 'llama3-8b'],
                       help='æ¨¡å‹åç§°')
    parser.add_argument('--samples', type=int, default=10,
                       help='ç¼–è¾‘æ ·æœ¬æ•?)
    parser.add_argument('--epochs', type=int, default=None,
                       help='è®­ç»ƒè½®æ•°ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨JSONé…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰')
    
    args = parser.parse_args()
    
    main(args.model, args.samples, args.epochs)
