# ä»£ç ä¿®å¤è¯´æ˜

## é—®é¢˜

### é—®é¢˜ 1ï¼š`model_config.py æœªæ‰¾åˆ°`
åœ¨è¿œç¨‹æœåŠ¡å™¨è¿è¡Œå®éªŒæ—¶æŠ¥é”™ï¼š`model_config.py æœªæ‰¾åˆ°`

**åŸå› åˆ†æï¼š**
`experiments/` ç›®å½•ä¸‹çš„è„šæœ¬ä½¿ç”¨ `sys.path.append('..')` æ¥å¯¼å…¥çˆ¶ç›®å½•çš„ `model_config.py`ï¼Œä½†è¿™ä¸ªç›¸å¯¹è·¯å¾„åœ¨æŸäº›æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­å¯èƒ½ä¸æ­£ç¡®ï¼Œå¯¼è‡´ Python æ— æ³•æ‰¾åˆ°æ¨¡å—ã€‚

### é—®é¢˜ 2ï¼šå¤šGPUè®¾å¤‡å†²çª
```
RuntimeError: Expected all tensors to be on the same device,
but found at least two devices, cuda:1 and cuda:0!
```

**åŸå› åˆ†æï¼š**
ä½¿ç”¨ `device_map="auto"` æ—¶ï¼Œaccelerate åº“ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¤šä¸ª GPUï¼Œå¯¼è‡´å¼ é‡åœ¨ä¸åŒè®¾å¤‡ä¸Šï¼Œè®¡ç®—æ—¶å‡ºç°è®¾å¤‡ä¸åŒ¹é…é”™è¯¯ã€‚

## ä¿®å¤å†…å®¹

### 1. ä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [experiments/evaluate_tokenedit.py](experiments/evaluate_tokenedit.py)
- [experiments/evaluate_all.py](experiments/evaluate_all.py)

**ä¿®æ”¹å†…å®¹ï¼š**
å°†ç®€å•çš„ `sys.path.append('..')` æ”¹ä¸ºæ›´å¯é çš„ç»å¯¹è·¯å¾„æ–¹æ³•ï¼š

```python
# ä¿®æ”¹å‰ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
import sys
sys.path.append('..')

# ä¿®æ”¹åï¼ˆæ›´å¯é ï¼‰
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)
```

**ä¼˜åŠ¿ï¼š**
- ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œä¸å—å½“å‰å·¥ä½œç›®å½•å½±å“
- `sys.path.insert(0, ...)` ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•ä¼˜å…ˆæœç´¢
- æ·»åŠ äº†æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•

### 2. ä¿®å¤å¤šGPUè®¾å¤‡å†²çªé—®é¢˜

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [model_config.py](model_config.py)

**ä¿®æ”¹å†…å®¹ï¼š**

#### a) ç§»é™¤ `device_map="auto"`
```python
# ä¿®æ”¹å‰ï¼ˆä¼šå¯¼è‡´å¤šGPUåˆ†é…ï¼‰
load_kwargs = {
    "device_map": "auto",  # âŒ ä¼šè‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
}

# ä¿®æ”¹åï¼ˆä½¿ç”¨å•GPUï¼‰
load_kwargs = {}  # âœ… ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
```

#### b) æ‰‹åŠ¨æ§åˆ¶è®¾å¤‡åˆ†é…
```python
# åŠ è½½æ¨¡å‹åˆ°CPU
print("  æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°CPU...")
model = AutoModelForCausalLM.from_pretrained(
    config['model_name'],
    **load_kwargs
)

# æ‰‹åŠ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
device = f"cuda:{device_id}"  # é»˜è®¤ cuda:0
print(f"  å°†æ¨¡å‹ç§»åŠ¨åˆ° {device}...")
model = model.to(device)
```

#### c) æ·»åŠ  device_id å‚æ•°
```python
def load_model_optimized(model_name: str, device_id=0):
    """
    åŠ è½½æ¨¡å‹ï¼ˆé’ˆå¯¹A800 80GBæ˜¾å­˜ä¼˜åŒ–ï¼‰

    Args:
        model_name: æ¨¡å‹åç§° (gpt2-xl, gpt-j-6b, llama3-8b)
        device_id: GPUè®¾å¤‡IDï¼Œé»˜è®¤ä¸º0ï¼ˆä½¿ç”¨å•GPUé¿å…å¤šè®¾å¤‡é—®é¢˜ï¼‰

    Returns:
        model, tokenizer, config
    """
```

**ä¼˜åŠ¿ï¼š**
- å¼ºåˆ¶ä½¿ç”¨å•ä¸ª GPUï¼Œé¿å…å¤šè®¾å¤‡å†²çª
- å¯ä»¥é€šè¿‡ `device_id` å‚æ•°æŒ‡å®šä½¿ç”¨å“ªä¸ª GPU
- æ›´æ¸…æ™°çš„è®¾å¤‡åˆ†é…æµç¨‹

### 3. ä¼˜åŒ– A800 GPU é…ç½®

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [model_config.py](model_config.py)

**ä¸»è¦æ”¹åŠ¨ï¼š**

#### a) æ›´æ–°æ–‡æ¡£æ³¨é‡Š
```python
# ä¿®æ”¹å‰
"""
é’ˆå¯¹A4000 (16GB)ä¼˜åŒ–
"""

# ä¿®æ”¹å
"""
é’ˆå¯¹A800 (80GB)ä¼˜åŒ– - æ— éœ€é‡åŒ–ï¼Œå¯ä»¥ä½¿ç”¨float16/bfloat16
"""
```

#### b) å…³é—­å¤§æ¨¡å‹çš„ 8bit é‡åŒ–
A800 æœ‰ 80GB æ˜¾å­˜ï¼Œè¶³å¤ŸåŠ è½½ GPT-J-6B å’Œ LLaMA-3-8B çš„å®Œæ•´ç²¾åº¦æ¨¡å‹ï¼š

```python
# GPT-J-6B é…ç½®
"gpt-j-6b": {
    "load_in_8bit": False,  # A800 80GBæ˜¾å­˜ï¼Œå¯ä»¥ä¸ç”¨é‡åŒ–
    "torch_dtype": "float16",  # ä½¿ç”¨float16ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    "memory_efficient": False,
}

# LLaMA-3-8B é…ç½®
"llama3-8b": {
    "load_in_8bit": False,  # A800 80GBæ˜¾å­˜ï¼Œå¯ä»¥ä¸ç”¨é‡åŒ–
    "torch_dtype": "float16",  # ä½¿ç”¨float16ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    "memory_efficient": False,
}
```

**ä¼˜åŠ¿ï¼š**
- æ— éœ€é‡åŒ–ï¼Œæ¨¡å‹ç²¾åº¦æ›´é«˜
- float16/bfloat16 æ¯” int8 é‡åŒ–æœ‰æ›´å¥½çš„è¡¨è¾¾èƒ½åŠ›
- è®­ç»ƒå’Œæ¨ç†æ•ˆæœå¯èƒ½æ›´å¥½

## ä½¿ç”¨è¯´æ˜

### åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œ

1. **åŒæ­¥ä»£ç **ï¼šå°†ä¿®æ”¹åçš„æ–‡ä»¶åŒæ­¥åˆ°æœåŠ¡å™¨
   ```bash
   # éœ€è¦åŒæ­¥çš„æ–‡ä»¶
   - experiments/evaluate_tokenedit.py
   - experiments/evaluate_all.py
   - model_config.py
   ```

2. **å‡†å¤‡æ•°æ®**ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰ï¼š
   ```bash
   python experiments/prepare_data.py
   ```

3. **è¿è¡Œè¯„ä¼°**ï¼š
   ```bash
   # å¿«é€Ÿæµ‹è¯•
   python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 50
   ```

### é¢„æœŸæ˜¾å­˜å ç”¨ï¼ˆA800ï¼‰

| æ¨¡å‹ | é…ç½® | é¢„ä¼°æ˜¾å­˜ |
|------|------|---------|
| GPT2-XL | float32 | ~6-8 GB |
| GPT-J-6B | float16 | ~12-15 GB |
| LLaMA-3-8B | float16 | ~16-20 GB |

A800 çš„ 80GB æ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œç”šè‡³å¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹ã€‚

## å¦‚æœä»ç„¶é‡åˆ°é—®é¢˜

### æ£€æŸ¥ Python è·¯å¾„
```bash
# åœ¨ Python ä¸­æ£€æŸ¥
python -c "import sys; print('\n'.join(sys.path))"
```

### æ‰‹åŠ¨è®¾ç½® PYTHONPATH
```bash
export PYTHONPATH=/path/to/TokenEdit-main:$PYTHONPATH
python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 50
```

### ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
```bash
cd /path/to/TokenEdit-main
python -m experiments.evaluate_tokenedit --model gpt2-xl --samples 20 --epochs 50
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆA800ï¼‰

ç”±äº A800 æ˜¾å­˜å……è¶³ï¼Œä½ å¯ä»¥ï¼š

1. **å¢åŠ  batch size**ï¼šåœ¨ `tokenedit/__init__.py` ä¸­è°ƒæ•´
2. **å¢åŠ ç›®æ ‡å±‚æ•°é‡**ï¼šåœ¨ `model_config.py` ä¸­ç¼–è¾‘ `target_layers`
3. **ä½¿ç”¨æ›´å¤§æ¨¡å‹**ï¼šå¯ä»¥å°è¯• LLaMA-3-70Bï¼ˆä½¿ç”¨ 8bit é‡åŒ–ï¼‰
4. **å¢åŠ è®­ç»ƒè½®æ•°**ï¼š`--epochs 100` æˆ–æ›´å¤š

## å¯¹æ¯”ï¼šA4000 vs A800 é…ç½®

| é…ç½®é¡¹ | A4000 (16GB) | A800 (80GB) |
|--------|-------------|-------------|
| GPT2-XL | float32 | float32 |
| GPT-J-6B | 8bité‡åŒ– | **float16**ï¼ˆæ— éœ€é‡åŒ–ï¼‰ |
| LLaMA-3-8B | 8bité‡åŒ– | **float16**ï¼ˆæ— éœ€é‡åŒ–ï¼‰ |
| æœ€å¤§æ¨¡å‹ | ~8Bå‚æ•° | ~70Bå‚æ•°ï¼ˆ8bitï¼‰ |

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

- âœ… [experiments/evaluate_tokenedit.py](experiments/evaluate_tokenedit.py) - ä¿®å¤å¯¼å…¥è·¯å¾„
- âœ… [experiments/evaluate_all.py](experiments/evaluate_all.py) - ä¿®å¤å¯¼å…¥è·¯å¾„
- âœ… [model_config.py](model_config.py) - A800 ä¼˜åŒ–é…ç½®
- âœ… FIXES.md - æœ¬æ–‡æ¡£

## ä¸‹ä¸€æ­¥

å°†ä¿®æ”¹åçš„ä»£ç åŒæ­¥åˆ°æœåŠ¡å™¨ï¼Œç„¶åè¿è¡Œï¼š

```bash
python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 50
```

åº”è¯¥å°±èƒ½æ­£å¸¸å·¥ä½œäº†ï¼ğŸš€
