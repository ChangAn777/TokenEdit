# ä»£ç ä¿®å¤è¯´æ˜

## é—®é¢˜

### é—®é¢˜ 1ï¼š`model_config.py æœªæ‰¾åˆ°`
åœ¨è¿œç¨‹æœåŠ¡å™¨è¿è¡Œå®éªŒæ—¶æŠ¥é”™ï¼š`model_config.py æœªæ‰¾åˆ°`

**åŸå› åˆ†æï¼š**
`experiments/` ç›®å½•ä¸‹çš„è„šæœ¬ä½¿ç”¨ `sys.path.append('..')` æ¥å¯¼å…¥çˆ¶ç›®å½•çš„ `model_config.py`ï¼Œä½†è¿™ä¸ªç›¸å¯¹è·¯å¾„åœ¨æŸäº›æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­å¯èƒ½ä¸æ­£ç¡®ï¼Œå¯¼è‡´ Python æ— æ³•æ‰¾åˆ°æ¨¡å—ã€‚

### é—®é¢˜ 2ï¼šå¤šGPUè®¾å¤‡å†²çª
```
RuntimeError: Expected all tensors to be on the same device,
but found at least two devices, cuda:1 and cuda:0!
```

**åŸå› åˆ†æï¼š**
ä½¿ç”¨ `device_map="auto"` æ—¶ï¼Œaccelerate åº“ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¤šä¸ª GPUï¼Œå¯¼è‡´å¼ é‡åœ¨ä¸åŒè®¾å¤‡ä¸Šï¼Œè®¡ç®—æ—¶å‡ºç°è®¾å¤‡ä¸åŒ¹é…é”™è¯¯ã€‚

### é—®é¢˜ 3ï¼šæœªä½¿ç”¨æ­£ç¡®çš„è¶…å‚æ•°é…ç½® âš ï¸ **æœ€é‡è¦ï¼**
```
ç¼–è¾‘æˆåŠŸç‡: 50%
æ³›åŒ–èƒ½åŠ›: 25%
```

**åŸå› åˆ†æï¼š**
å®éªŒä»£ç **æ²¡æœ‰åŠ è½½** `hparams/TokenEdit/gpt2-xl.json` é…ç½®æ–‡ä»¶ï¼Œè€Œæ˜¯ä½¿ç”¨ç¡¬ç¼–ç çš„é”™è¯¯å‚æ•°ï¼š

| å‚æ•° | JSONé…ç½®å€¼ | ä»£ç ä½¿ç”¨çš„å€¼ | å½±å“ |
|------|-----------|-------------|------|
| `learning_rate` | `0.1` | `0.001` | âŒ å·®äº†100å€ï¼Œå¯¼è‡´è®­ç»ƒå‡ ä¹æ— æ•ˆ |
| `num_epochs` | `150` | `50` | âŒ è®­ç»ƒä¸è¶³ |
| `target_layers` | `[13,14,15,16,17]` | `[15-24]` | âŒ å±‚æ•°ä¸åŒ¹é… |
| `w_edit` | `1.5` | `1.0` (é»˜è®¤) | âŒ ç¼–è¾‘æƒé‡ä¸è¶³ |
| `w_suppress` | `0.5` | `0.5` (é»˜è®¤) | âœ… æ­£ç¡® |
| `w_ortho` | `0.1` | `0.3` (é»˜è®¤) | âŒ æ­£äº¤çº¦æŸè¿‡å¼º |

## ä¿®å¤å†…å®¹

### 1. ä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [experiments/evaluate_tokenedit.py](experiments/evaluate_tokenedit.py)
- [experiments/evaluate_all.py](experiments/evaluate_all.py)

**ä¿®æ”¹å†…å®¹ï¼š**
å°†ç®€å•çš„ `sys.path.append('..')` æ”¹ä¸ºæ›´å¯é çš„ç»å¯¹è·¯å¾„æ–¹æ³•ï¼š

```python
# ä¿®æ”¹å‰ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
import sys
sys.path.append('..')

# ä¿®æ”¹åï¼ˆæ›´å¯é ï¼‰
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)
```

**ä¼˜åŠ¿ï¼š**
- ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œä¸å—å½“å‰å·¥ä½œç›®å½•å½±å“
- `sys.path.insert(0, ...)` ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•ä¼˜å…ˆæœç´¢
- æ·»åŠ äº†æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•

### 2. ä¿®å¤å¤šGPUè®¾å¤‡å†²çªé—®é¢˜

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [model_config.py](model_config.py)

**ä¿®æ”¹å†…å®¹ï¼š**

#### a) ç§»é™¤ `device_map="auto"`
```python
# ä¿®æ”¹å‰ï¼ˆä¼šå¯¼è‡´å¤šGPUåˆ†é…ï¼‰
load_kwargs = {
    "device_map": "auto",  # âŒ ä¼šè‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
}

# ä¿®æ”¹åï¼ˆä½¿ç”¨å•GPUï¼‰
load_kwargs = {}  # âœ… ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
```

#### b) æ‰‹åŠ¨æ§åˆ¶è®¾å¤‡åˆ†é…
```python
# åŠ è½½æ¨¡å‹åˆ°CPU
print("  æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°CPU...")
model = AutoModelForCausalLM.from_pretrained(
    config['model_name'],
    **load_kwargs
)

# æ‰‹åŠ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
device = f"cuda:{device_id}"  # é»˜è®¤ cuda:0
print(f"  å°†æ¨¡å‹ç§»åŠ¨åˆ° {device}...")
model = model.to(device)
```

#### c) æ·»åŠ  device_id å‚æ•°
```python
def load_model_optimized(model_name: str, device_id=0):
    """
    åŠ è½½æ¨¡å‹ï¼ˆé’ˆå¯¹A800 80GBæ˜¾å­˜ä¼˜åŒ–ï¼‰

    Args:
        model_name: æ¨¡å‹åç§° (gpt2-xl, gpt-j-6b, llama3-8b)
        device_id: GPUè®¾å¤‡IDï¼Œé»˜è®¤ä¸º0ï¼ˆä½¿ç”¨å•GPUé¿å…å¤šè®¾å¤‡é—®é¢˜ï¼‰

    Returns:
        model, tokenizer, config
    """
```

**ä¼˜åŠ¿ï¼š**
- å¼ºåˆ¶ä½¿ç”¨å•ä¸ª GPUï¼Œé¿å…å¤šè®¾å¤‡å†²çª
- å¯ä»¥é€šè¿‡ `device_id` å‚æ•°æŒ‡å®šä½¿ç”¨å“ªä¸ª GPU
- æ›´æ¸…æ™°çš„è®¾å¤‡åˆ†é…æµç¨‹

### 3. ä¼˜åŒ– A800 GPU é…ç½®

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [model_config.py](model_config.py)

**ä¸»è¦æ”¹åŠ¨ï¼š**

#### a) æ›´æ–°æ–‡æ¡£æ³¨é‡Š
```python
# ä¿®æ”¹å‰
"""
é’ˆå¯¹A4000 (16GB)ä¼˜åŒ–
"""

# ä¿®æ”¹å
"""
é’ˆå¯¹A800 (80GB)ä¼˜åŒ– - æ— éœ€é‡åŒ–ï¼Œå¯ä»¥ä½¿ç”¨float16/bfloat16
"""
```

#### b) å…³é—­å¤§æ¨¡å‹çš„ 8bit é‡åŒ–
A800 æœ‰ 80GB æ˜¾å­˜ï¼Œè¶³å¤ŸåŠ è½½ GPT-J-6B å’Œ LLaMA-3-8B çš„å®Œæ•´ç²¾åº¦æ¨¡å‹ï¼š

```python
# GPT-J-6B é…ç½®
"gpt-j-6b": {
    "load_in_8bit": False,  # A800 80GBæ˜¾å­˜ï¼Œå¯ä»¥ä¸ç”¨é‡åŒ–
    "torch_dtype": "float16",  # ä½¿ç”¨float16ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    "memory_efficient": False,
}

# LLaMA-3-8B é…ç½®
"llama3-8b": {
    "load_in_8bit": False,  # A800 80GBæ˜¾å­˜ï¼Œå¯ä»¥ä¸ç”¨é‡åŒ–
    "torch_dtype": "float16",  # ä½¿ç”¨float16ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    "memory_efficient": False,
}
```

**ä¼˜åŠ¿ï¼š**
- æ— éœ€é‡åŒ–ï¼Œæ¨¡å‹ç²¾åº¦æ›´é«˜
- float16/bfloat16 æ¯” int8 é‡åŒ–æœ‰æ›´å¥½çš„è¡¨è¾¾èƒ½åŠ›
- è®­ç»ƒå’Œæ¨ç†æ•ˆæœå¯èƒ½æ›´å¥½

### 4. ä¿®å¤è¶…å‚æ•°åŠ è½½é—®é¢˜ â­ **æ ¸å¿ƒä¿®å¤**

**ä¿®æ”¹æ–‡ä»¶ï¼š**
- [experiments/evaluate_tokenedit.py](experiments/evaluate_tokenedit.py)
- [experiments/evaluate_all.py](experiments/evaluate_all.py)

**ä¿®æ”¹å†…å®¹ï¼š**

#### a) æ·»åŠ  `load_hparams_from_json` å‡½æ•°
```python
def load_hparams_from_json(model_name: str, hparams_dir: str = "hparams/TokenEdit"):
    """ä»JSONæ–‡ä»¶åŠ è½½è¶…å‚æ•°é…ç½®"""
    hparams_path = Path(hparams_dir) / f"{model_name}.json"

    if not hparams_path.exists():
        print(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {hparams_path}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        return TokenEditHyperParams(model_name=model_name)

    print(f"âœ“ ä» {hparams_path} åŠ è½½é…ç½®")

    with open(hparams_path, 'r') as f:
        config = json.load(f)

    # æ‰“å°å…³é”®é…ç½®
    print(f"  é…ç½®å‚æ•°:")
    print(f"    - target_layers: {config.get('target_layers', 'æœªè®¾ç½®')}")
    print(f"    - num_epochs: {config.get('num_epochs', 100)}")
    print(f"    - learning_rate: {config.get('learning_rate', 0.001)}")

    return TokenEditHyperParams(**config)
```

#### b) ä¿®æ”¹ç¼–è¾‘å™¨åˆ›å»ºä»£ç 
```python
# âŒ ä¿®æ”¹å‰ï¼ˆç¡¬ç¼–ç é”™è¯¯å‚æ•°ï¼‰
hparams = TokenEditHyperParams(
    model_name=model_name,
    num_epochs=num_epochs,
    learning_rate=0.001,  # é”™è¯¯ï¼åº”è¯¥æ˜¯ 0.1
    target_layers=config['target_layers'],
    device="cuda" if torch.cuda.is_available() else "cpu",
    verbose=False
)

# âœ… ä¿®æ”¹åï¼ˆä»JSONåŠ è½½æ­£ç¡®å‚æ•°ï¼‰
hparams = load_hparams_from_json(model_name)

# å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†num_epochsï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
if num_epochs is not None:
    hparams.num_epochs = num_epochs
    print(f"  è¦†ç›– num_epochs ä¸º: {num_epochs}")

hparams.device = "cuda" if torch.cuda.is_available() else "cpu"
hparams.verbose = False
```

**ä¼˜åŠ¿ï¼š**
- âœ… ä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡ï¼ˆ0.1 è€Œä¸æ˜¯ 0.001ï¼‰
- âœ… ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒè½®æ•°ï¼ˆ150 è€Œä¸æ˜¯ 50ï¼‰
- âœ… ä½¿ç”¨æ­£ç¡®çš„ç›®æ ‡å±‚ï¼ˆ[13-17] è€Œä¸æ˜¯ [15-24]ï¼‰
- âœ… ä½¿ç”¨æ­£ç¡®çš„æŸå¤±æƒé‡
- âœ… å¯ä»¥é€šè¿‡ä¿®æ”¹ JSON æ–‡ä»¶å¿«é€Ÿè°ƒæ•´å‚æ•°

## ä½¿ç”¨è¯´æ˜

### åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œ

1. **åŒæ­¥ä»£ç **ï¼šå°†ä¿®æ”¹åçš„æ–‡ä»¶åŒæ­¥åˆ°æœåŠ¡å™¨
   ```bash
   # éœ€è¦åŒæ­¥çš„æ–‡ä»¶
   - experiments/evaluate_tokenedit.py
   - experiments/evaluate_all.py
   - model_config.py
   - hparams/TokenEdit/gpt2-xl.json  # ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨
   ```

2. **å‡†å¤‡æ•°æ®**ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰ï¼š
   ```bash
   python experiments/prepare_data.py
   ```

3. **è¿è¡Œè¯„ä¼°**ï¼š
   ```bash
   # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ï¼ˆlearning_rate=0.1, num_epochs=150ï¼‰
   python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20

   # å¦‚æœæƒ³è¦†ç›–è®­ç»ƒè½®æ•°
   python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 100
   ```

4. **æŸ¥çœ‹è¾“å‡º**ï¼š
   ```
   âœ“ ä» hparams/TokenEdit/gpt2-xl.json åŠ è½½é…ç½®
     é…ç½®å‚æ•°:
       - target_layers: [13, 14, 15, 16, 17]
       - num_epochs: 150
       - learning_rate: 0.1
       - w_edit: 1.5
       - w_suppress: 0.5
   ```

### é¢„æœŸæ•ˆæœæ”¹å–„

ä½¿ç”¨æ­£ç¡®çš„å‚æ•°åï¼Œé¢„æœŸæŒ‡æ ‡ä¼šå¤§å¹…æå‡ï¼š

| æŒ‡æ ‡ | ä¹‹å‰ï¼ˆé”™è¯¯å‚æ•°ï¼‰ | é¢„æœŸï¼ˆæ­£ç¡®å‚æ•°ï¼‰ |
|------|----------------|----------------|
| ç¼–è¾‘æˆåŠŸç‡ | 50% | **80-95%** |
| æ³›åŒ–èƒ½åŠ› | 25% | **70-90%** |

### è°ƒæ•´è¶…å‚æ•°

å¦‚æœæ•ˆæœä»ä¸ç†æƒ³ï¼Œå¯ä»¥ç¼–è¾‘ `hparams/TokenEdit/gpt2-xl.json`ï¼š

```json
{
  "learning_rate": 0.1,      // å°è¯• 0.05 - 0.2
  "num_epochs": 150,          // å°è¯• 100 - 200
  "w_edit": 1.5,             // ç¼–è¾‘æŸå¤±æƒé‡
  "w_suppress": 0.5,         // æŠ‘åˆ¶æŸå¤±æƒé‡
  "w_ortho": 0.1,            // æ­£äº¤çº¦æŸæƒé‡
  "target_layers": [13, 14, 15, 16, 17]
}
```

### é¢„æœŸæ˜¾å­˜å ç”¨ï¼ˆA800ï¼‰

| æ¨¡å‹ | é…ç½® | é¢„ä¼°æ˜¾å­˜ |
|------|------|---------|
| GPT2-XL | float32 | ~6-8 GB |
| GPT-J-6B | float16 | ~12-15 GB |
| LLaMA-3-8B | float16 | ~16-20 GB |

A800 çš„ 80GB æ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œç”šè‡³å¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹ã€‚

## å¦‚æœä»ç„¶é‡åˆ°é—®é¢˜

### æ£€æŸ¥ Python è·¯å¾„
```bash
# åœ¨ Python ä¸­æ£€æŸ¥
python -c "import sys; print('\n'.join(sys.path))"
```

### æ‰‹åŠ¨è®¾ç½® PYTHONPATH
```bash
export PYTHONPATH=/path/to/TokenEdit-main:$PYTHONPATH
python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 50
```

### ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
```bash
cd /path/to/TokenEdit-main
python -m experiments.evaluate_tokenedit --model gpt2-xl --samples 20 --epochs 50
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆA800ï¼‰

ç”±äº A800 æ˜¾å­˜å……è¶³ï¼Œä½ å¯ä»¥ï¼š

1. **å¢åŠ  batch size**ï¼šåœ¨ `tokenedit/__init__.py` ä¸­è°ƒæ•´
2. **å¢åŠ ç›®æ ‡å±‚æ•°é‡**ï¼šåœ¨ `model_config.py` ä¸­ç¼–è¾‘ `target_layers`
3. **ä½¿ç”¨æ›´å¤§æ¨¡å‹**ï¼šå¯ä»¥å°è¯• LLaMA-3-70Bï¼ˆä½¿ç”¨ 8bit é‡åŒ–ï¼‰
4. **å¢åŠ è®­ç»ƒè½®æ•°**ï¼š`--epochs 100` æˆ–æ›´å¤š

## å¯¹æ¯”ï¼šA4000 vs A800 é…ç½®

| é…ç½®é¡¹ | A4000 (16GB) | A800 (80GB) |
|--------|-------------|-------------|
| GPT2-XL | float32 | float32 |
| GPT-J-6B | 8bité‡åŒ– | **float16**ï¼ˆæ— éœ€é‡åŒ–ï¼‰ |
| LLaMA-3-8B | 8bité‡åŒ– | **float16**ï¼ˆæ— éœ€é‡åŒ–ï¼‰ |
| æœ€å¤§æ¨¡å‹ | ~8Bå‚æ•° | ~70Bå‚æ•°ï¼ˆ8bitï¼‰ |

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

- âœ… [experiments/evaluate_tokenedit.py](experiments/evaluate_tokenedit.py) - ä¿®å¤å¯¼å…¥è·¯å¾„
- âœ… [experiments/evaluate_all.py](experiments/evaluate_all.py) - ä¿®å¤å¯¼å…¥è·¯å¾„
- âœ… [model_config.py](model_config.py) - A800 ä¼˜åŒ–é…ç½®
- âœ… FIXES.md - æœ¬æ–‡æ¡£

## ä¸‹ä¸€æ­¥

å°†ä¿®æ”¹åçš„ä»£ç åŒæ­¥åˆ°æœåŠ¡å™¨ï¼Œç„¶åè¿è¡Œï¼š

```bash
python experiments/evaluate_tokenedit.py --model gpt2-xl --samples 20 --epochs 50
```

åº”è¯¥å°±èƒ½æ­£å¸¸å·¥ä½œäº†ï¼ğŸš€
