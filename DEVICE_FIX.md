# å•GPUè®¾å¤‡é”™è¯¯ä¿®å¤

## é—®é¢˜æè¿°

```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!
```

è¿™ä¸ªé”™è¯¯å‘ç”Ÿåœ¨ä½¿ç”¨ `device_map="auto"` æ—¶ï¼Œå³ä½¿åªæœ‰ä¸€å—GPUï¼Œaccelerateä¹Ÿä¼šå°è¯•å°†æ¨¡å‹åˆ†æ•£åˆ°å¤šä¸ª"è™šæ‹Ÿ"è®¾å¤‡ä¸Šã€‚

## ä¿®å¤å†…å®¹

### 1. ä¿®å¤æ¨¡å‹åŠ è½½ ([model_config.py](model_config.py:43-105))

**å…³é”®æ”¹åŠ¨ï¼š**
- ç§»é™¤ `device_map="auto"`
- æ‰‹åŠ¨æ§åˆ¶è®¾å¤‡åˆ†é…
- æ·»åŠ  `torch.cuda.empty_cache()` æ¸…ç©ºç¼“å­˜
- æ˜¾å¼è°ƒç”¨ `model.to(device)` å’Œ `model.eval()`

```python
# æ—§ä»£ç 
load_kwargs = {
    "device_map": "auto",  # â† é—®é¢˜æ‰€åœ¨
}

# æ–°ä»£ç 
device = "cuda" if torch.cuda.is_available() else "cpu"
if torch.cuda.is_available():
    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜

load_kwargs = {}  # ä¸ä½¿ç”¨ device_map

# å…ˆåŠ è½½åˆ°CPUï¼Œå†æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
model = AutoModelForCausalLM.from_pretrained(config['model_name'], **load_kwargs)
model = model.to(device)
model.eval()
```

### 2. ä¼˜åŒ–è·¯ç”±æ³¨å†Œ ([tokenedit/prompt_router.py](tokenedit/prompt_router.py:48-73))

**å…³é”®æ”¹åŠ¨ï¼š**
- åªåœ¨ `use_embedding_routing=true` æ—¶è®¡ç®— embeddings
- é¿å…ä¸å¿…è¦çš„æ¨¡å‹å‰å‘ä¼ æ’­

```python
# æ—§ä»£ç 
def register_edit(self, edit_id, subject, relation):
    text = f"{subject} {relation}"
    inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
    outputs = self.model(**inputs, output_hidden_states=True)
    embedding = outputs.hidden_states[-1].mean(dim=1)
    self.edit_embeddings[edit_id] = embedding  # â† æ€»æ˜¯è®¡ç®—

# æ–°ä»£ç 
def register_edit(self, edit_id, subject, relation):
    # åªåœ¨ä½¿ç”¨embeddingè·¯ç”±æ—¶è®¡ç®—
    if self.hparams.use_embedding_routing:
        text = f"{subject} {relation}"
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            embedding = outputs.hidden_states[-1].mean(dim=1)
        self.edit_embeddings[edit_id] = embedding
```

## ä¼˜åŠ¿

1. **é¿å…è®¾å¤‡å†²çª** - æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
2. **èŠ‚çœæ˜¾å­˜** - ä¸è®¡ç®—ä¸å¿…è¦çš„ embeddings
3. **åŠ å¿«é€Ÿåº¦** - è·³è¿‡ embedding è®¡ç®—æ­¥éª¤
4. **æ›´ç¨³å®š** - å®Œå…¨æ§åˆ¶è®¾å¤‡åˆ†é…

## éªŒè¯

ä¿®å¤åï¼Œæ‚¨åº”è¯¥èƒ½æˆåŠŸè¿è¡Œï¼š

```bash
python test_tokenedit_debug.py gpt2-xl
```

é¢„æœŸè¾“å‡ºï¼š
```
[1/5] åŠ è½½æ¨¡å‹...
åŠ è½½æ¨¡å‹: gpt2-xl
  8bité‡åŒ–: False
  ç²¾åº¦: float32
æ¨¡å‹åŠ è½½å®Œæˆ
  è®¾å¤‡: cuda
  æ˜¾å­˜å ç”¨: X.XXGB (å·²åˆ†é…) / Y.YYGB (å·²ä¿ç•™)

[2/5] Load hyperparams...
...
```

## å…¶ä»–æ³¨æ„äº‹é¡¹

### å¦‚æœä»ç„¶å‡ºç°è®¾å¤‡é”™è¯¯

æ£€æŸ¥æ‚¨çš„ hparams é…ç½®ï¼š

```json
{
    "device": "cuda",  // ç¡®ä¿æ˜¯ "cuda" æˆ– "cpu"
    "use_embedding_routing": false  // æ¨èè®¾ä¸º false
}
```

### å¦‚æœæ˜¾å­˜ä¸è¶³

å¯¹äº A800 (80GB)ï¼ŒGPT-2-XL (1.5B) åº”è¯¥å®Œå…¨å¯ä»¥åŠ è½½ã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. **ä½¿ç”¨æ›´å°‘çš„ç›®æ ‡å±‚ï¼š**
   ```python
   "target_layers": [13, 14, 15]  # åªç”¨3å±‚è€Œä¸æ˜¯5å±‚
   ```

2. **å‡å°‘ batch sizeï¼š**
   ```python
   "batch_size": 1  # å·²ç»æ˜¯1äº†
   ```

3. **ä½¿ç”¨8bité‡åŒ–ï¼š**
   ```python
   "load_in_8bit": true  # åœ¨ model_config.py ä¸­è®¾ç½®
   ```

## å®Œæ•´ä¿®å¤åˆ—è¡¨

- [x] ä¿®å¤ model_config.py çš„è®¾å¤‡åˆ†é…
- [x] ä¼˜åŒ– prompt_router.py çš„ embedding è®¡ç®—
- [x] æ·»åŠ ç¼“å­˜æ¸…ç†
- [x] ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡

## ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å¯ä»¥é‡æ–°è¿è¡Œæµ‹è¯•ï¼š

```bash
cd /home/dengjiaming/TokenEdit
python test_tokenedit_debug.py gpt2-xl
```

å¦‚æœæˆåŠŸï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ï¼š
- âœ… æ¨¡å‹åŠ è½½åˆ°å•ä¸ª cuda è®¾å¤‡
- âœ… è·¯ç”±æ­£ç¡®å·¥ä½œ
- âœ… ç¼–è¾‘æˆåŠŸåº”ç”¨
- âœ… æ¨ç†äº§ç”Ÿæ­£ç¡®è¾“å‡º

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€
