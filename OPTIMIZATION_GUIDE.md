# TokenEdit è°ƒæ•´æŒ‡å—

## ğŸ“Š å½“å‰æµ‹è¯•ç»“æœåˆ†æ

### âœ… æˆåŠŸéƒ¨åˆ†
- è·¯ç”±æ£€æµ‹æˆåŠŸï¼ˆæ‰€æœ‰æµ‹è¯•ï¼‰
- ä¸»ä½“ä½ç½®æ£€æµ‹å‡†ç¡®
- ç¬¬1ä¸ªæµ‹è¯•ç”¨ä¾‹æˆåŠŸï¼š"The capital of France is Lyon"

### âŒ é—®é¢˜
1. **è¿‡åº¦è§¦å‘** - "France is in" ä¸åº”è¯¥è§¦å‘ï¼ˆæ—  capital å…³ç³»ï¼‰
2. **ï¿½ï¿½ç­”å¤±è´¥** - "What is the capital of France?" æ²¡æœ‰æ­£ç¡®å›ç­”

---

## ğŸ”§ é—®é¢˜ 1: è·¯ç”±è¿‡äºå®½æ¾

### ç°è±¡
```
è¾“å…¥: France is in
âœ“ è§¦å‘ç¼–è¾‘ #0: France -> Lyon  â† ä¸åº”è¯¥è§¦å‘ï¼
```

### åŸå› 
å½“å‰è·¯ç”±åªè¦åŒ…å« "France" å°±è§¦å‘ï¼Œæ²¡æœ‰ä¸¥æ ¼æ£€æŸ¥ capital å…³ç³»ã€‚

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ A: å¢å¼ºå…³ç³»æ¨¡æ¿åŒ¹é…ï¼ˆæ¨èï¼‰

ä¿®æ”¹ [tokenedit/prompt_router.py](tokenedit/prompt_router.py) çš„è·¯ç”±é€»è¾‘ï¼š

```python
def route(self, prompt: str, prompt_embedding: Optional[torch.Tensor] = None) -> Optional[int]:
    if len(self.edit_embeddings) == 0:
        return None

    # æ–¹æ³•1: å…³ç³»æ¨¡æ¿åŒ¹é…ï¼ˆæ›´ä¸¥æ ¼ï¼‰
    if self.hparams.use_template_routing:
        for edit_id, info in self.edit_info.items():
            subject = info["subject"]
            relation = info["relation"]

            # æ£€æŸ¥ä¸»ä½“
            if subject.lower() in prompt.lower():
                # æ£€æŸ¥å…³ç³»å…³é”®è¯
                templates = self.relation_templates.get(relation, [])
                relation_found = False
                for template in templates:
                    if template.lower() in prompt.lower():
                        relation_found = True
                        break

                # åªæœ‰ä¸»ä½“å’Œå…³ç³»éƒ½åŒ¹é…æ‰è§¦å‘
                if relation_found:
                    return edit_id

    # æ–¹æ³•2: Embeddingç›¸ä¼¼åº¦ï¼ˆä½œä¸ºå¤‡é€‰ï¼Œä½†æé«˜é˜ˆå€¼ï¼‰
    if self.hparams.use_embedding_routing:
        # ... ç°æœ‰ä»£ç 
        pass

    return None
```

#### æ–¹æ¡ˆ B: å…³é—­ Embedding è·¯ç”±ï¼ˆæ›´ä¸¥æ ¼ï¼‰

```python
# åœ¨ hparams é…ç½®ä¸­
use_embedding_routing: bool = False  # åªä½¿ç”¨æ¨¡æ¿åŒ¹é…
use_template_routing: bool = True
```

---

## ğŸ”§ é—®é¢˜ 2: é—®ç­”å½¢å¼è¾“å‡ºå¤±è´¥

### ç°è±¡
```
è¾“å…¥: What is the capital of France?
è¾“å‡º: What is the capital of France?

France is the capital of France.  â† é”™è¯¯å›ç­”
```

### åŸå› 
ç¼–è¾‘åªæ”¹å˜ "France" çš„è¡¨ç¤ºï¼Œä½†æ¨¡å‹ç”Ÿæˆæ—¶å¯èƒ½ï¼š
1. é‡å¤é—®é¢˜
2. æ²¡æœ‰ç›´æ¥ç”Ÿæˆç­”æ¡ˆ

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ A: ä¼˜åŒ–è®­ç»ƒæ•°æ®ï¼ˆæ¨èï¼‰

åœ¨ [tokenedit/prompt_closure.py](tokenedit/prompt_closure.py) ä¸­æ·»åŠ é—®é¢˜å½¢å¼çš„è®­ç»ƒæ ·æœ¬ï¼š

```python
"capital": {
    "forward": [
        "The capital of {subject} is",
        "{subject}'s capital is",
        "What is the capital of {subject}?",  # æ·»åŠ é—®é¢˜å½¢å¼
    ],
    "backward": [...],
    "judge": [...],
    "distract": [...]
}
```

#### æ–¹æ¡ˆ B: è°ƒæ•´ç”Ÿæˆç­–ç•¥

åœ¨æ¨ç†æ—¶ä½¿ç”¨ä¸åŒçš„è§£ç å‚æ•°ï¼š

```python
# åœ¨ inference å‡½æ•°ä¸­
output_ids = self.model.generate(
    inputs['input_ids'],
    max_new_tokens=max_new_tokens,
    do_sample=False,
    temperature=0.7,  # æ·»åŠ æ¸©åº¦
    top_p=0.9,        # æ·»åŠ  top-p é‡‡æ ·
    pad_token_id=self.tokenizer.eos_token_id
)
```

#### æ–¹æ¡ˆ C: å¢åŠ è®­ç»ƒè½®æ•°

```python
# åœ¨ hparams ä¸­
num_epochs: int = 50  # ä» 20 å¢åŠ åˆ° 50
```

---

## ğŸ¯ ç«‹å³å¯ç”¨çš„è°ƒæ•´æ–¹æ¡ˆ

### æ­¥éª¤ 1: ä¿®æ”¹è·¯ç”±é…ç½®

åˆ›å»ºæ–°çš„ hparams æ–‡ä»¶ `hparams/TokenEdit/gpt2-xl-strict.json`:

```json
{
    "model_name": "gpt2-xl",
    "target_layers": [13, 14, 15, 16, 17],

    "token_init_method": "random",
    "token_init_std": 0.1,
    "learnable_gates": true,
    "use_low_rank": false,

    "num_epochs": 50,
    "learning_rate": 0.001,
    "batch_size": 1,

    "w_edit": 1.0,
    "w_suppress": 0.5,
    "w_ortho": 0.1,
    "w_local": 0.1,

    "routing_threshold": 0.5,
    "use_embedding_routing": false,  // å…³é—­ embedding è·¯ç”±
    "use_template_routing": true,    // åªä½¿ç”¨æ¨¡æ¿åŒ¹é…

    "use_forward": true,
    "use_backward": true,
    "use_judge": false,
    "use_distract": false,

    "device": "cuda",
    "verbose": true
}
```

### æ­¥éª¤ 2: å¢å¼ºå…³ç³»æ¨¡æ¿

ä¿®æ”¹ [tokenedit/prompt_router.py](tokenedit/prompt_router.py:31-40):

```python
def _load_relation_templates(self) -> Dict[str, List[str]]:
    return {
        "capital": [
            "capital",
            "capital of",
            "capital city",
            "é¦–éƒ½"
        ],
        "president": ["president", "æ€»ç»Ÿ", "leader of"],
        # ...
    }
```

### æ­¥éª¤ 3: æ·»åŠ é—®é¢˜å½¢å¼è®­ç»ƒæ ·æœ¬

ä¿®æ”¹ [tokenedit/prompt_closure.py](tokenedit/prompt_closure.py:19-24):

```python
"forward": [
    "The capital of {subject} is",
    "{subject}'s capital is",
    "What is the capital of {subject}?",
    "Tell me the capital of {subject}",
],
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### ä¿®å¤åé¢„æœŸç»“æœ

| æµ‹è¯•ç”¨ä¾‹ | å½“å‰ | ä¿®å¤å | è¯´æ˜ |
|---------|------|--------|------|
| "The capital of France is" | âœ… Lyon | âœ… Lyon | ä¿æŒ |
| "France is in" | âŒ è§¦å‘ | âœ… ä¸è§¦å‘ | ä¿®å¤ |
| "What is the capital of France?" | âŒ é”™è¯¯ | âœ… Lyon | ä¿®å¤ |
| "France's capital city is" | âœ… Lyon | âœ… Lyon | ä¿æŒ |

---

## ğŸš€ å¿«é€Ÿå®æ–½

### é€‰é¡¹ 1: æœ€å°è°ƒæ•´ï¼ˆæ¨èï¼‰

åªéœ€ä¿®æ”¹ hparams é…ç½®ï¼š

```json
{
    "use_embedding_routing": false,
    "num_epochs": 50
}
```

### é€‰é¡¹ 2: å®Œæ•´ä¼˜åŒ–

1. ä¿®æ”¹è·¯ç”±é€»è¾‘
2. å¢å¼ºå…³ç³»æ¨¡æ¿
3. æ·»åŠ é—®é¢˜å½¢å¼è®­ç»ƒæ ·æœ¬
4. å¢åŠ è®­ç»ƒè½®æ•°

---

## ğŸ’¡ è°ƒè¯•å»ºè®®

### 1. éªŒè¯è·¯ç”±é€»è¾‘

```python
# åœ¨æµ‹è¯•è„šæœ¬ä¸­æ·»åŠ 
test_prompts = [
    ("The capital of France is", True),   # åº”è¯¥è§¦å‘
    ("France is in", False),              # ä¸åº”è¯¥è§¦å‘
    ("What is the capital of France?", True),  # åº”è¯¥è§¦å‘
]

for prompt, should_trigger in test_prompts:
    edit_id = editor.router.route(prompt, prompt_emb)
    if should_trigger:
        assert edit_id is not None, f"åº”è¯¥è§¦å‘ä½†æ²¡è§¦å‘: {prompt}"
    else:
        assert edit_id is None, f"ä¸åº”è¯¥è§¦å‘ä½†è§¦å‘äº†: {prompt}"
```

### 2. æ£€æŸ¥è®­ç»ƒæ ·æœ¬

```python
# æŸ¥çœ‹ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬
closure = editor.closure_gen.generate(
    subject="France",
    relation="capital",
    new_object="Lyon",
    old_object="Paris"
)

print("Forward samples:", closure['forward'])
print("Backward samples:", closure['backward'])
```

### 3. ç›‘æ§è®­ç»ƒæŸå¤±

```python
# è§‚å¯Ÿ loss æ›²çº¿
# Edit loss åº”è¯¥é™åˆ° 0.1 ä»¥ä¸‹
# Suppress loss åº”è¯¥ç¨³å®šåœ¨ 0.5-1.0
```

---

## ğŸ“ æ€»ç»“

### å½“å‰é—®é¢˜
1. âŒ è·¯ç”±è¿‡äºå®½æ¾ï¼ˆä¸éœ€è¦ç¼–è¾‘çš„è¾“å…¥ä¹Ÿè§¦å‘ï¼‰
2. âŒ é—®ç­”å½¢å¼è¾“å‡ºä¸æ­£ç¡®

### æ¨èè°ƒæ•´ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
1. **ç«‹å³æ‰§è¡Œ** - è®¾ç½® `use_embedding_routing: false`
2. **é‡è¦** - å¢åŠ è®­ç»ƒè½®æ•°åˆ° 50
3. **å»ºè®®** - æ·»åŠ é—®é¢˜å½¢å¼è®­ç»ƒæ ·æœ¬
4. **å¯é€‰** - å¢å¼ºå…³ç³»æ¨¡æ¿åŒ¹é…

### é¢„æœŸæ”¹è¿›
- âœ… æ›´ç²¾ç¡®çš„è·¯ç”±ï¼ˆåªåœ¨éœ€è¦æ—¶è§¦å‘ï¼‰
- âœ… æ­£ç¡®çš„é—®ç­”è¾“å‡º
- âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
