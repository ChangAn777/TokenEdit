{
  "model_name": "gpt2-xl",
  "target_layers": [13, 14, 15, 16, 17],
  "token_init_method": "random",
  "token_init_std": 0.2,
  "learnable_gates": true,
  "use_low_rank": false,
  "token_rank": 64,
  "num_epochs": 150,
  "learning_rate": 0.05,
  "batch_size": 1,
  "optimizer": "adam",
  "scheduler": "constant",
  "warmup_steps": 0,
  "w_edit": 10.0,
  "w_suppress": 0.5,
  "w_ortho": 0.1,
  "w_local": 0.2,
  "ortho_prompt_lambda": 1.0,
  "ortho_token_lambda": 1.0,
  "ortho_method": "inner_product",
  "routing_threshold": 0.3,
  "use_embedding_routing": true,
  "use_template_routing": true,
  "routing_aggregation": "max",
  "use_forward": true,
  "use_backward": true,
  "use_judge": true,
  "use_distract": true,
  "num_paraphrase": 5,
  "device": "cuda",
  "seed": 42,
  "verbose": true,
  "save_checkpoints": true
}