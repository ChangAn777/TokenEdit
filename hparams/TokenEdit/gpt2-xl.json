{
    "model_name": "gpt2-xl",
    "target_layers": [13, 14, 15, 16, 17],
    "token_init_method": "random",
    "token_init_std": 0.01,
    "learnable_gates": true,
    "use_low_rank": false,
    "token_rank": 64,
    "num_epochs": 100,
    "learning_rate": 0.001,
    "batch_size": 4,
    "optimizer": "adam",
    "scheduler": "cosine",
    "warmup_steps": 10,
    "w_edit": 1.0,
    "w_suppress": 0.5,
    "w_ortho": 0.3,
    "w_local": 0.2,
    "ortho_prompt_lambda": 1.0,
    "ortho_token_lambda": 1.0,
    "ortho_method": "inner_product",
    "routing_threshold": 0.8,
    "use_embedding_routing": true,
    "use_template_routing": true,
    "routing_aggregation": "max",
    "use_forward": true,
    "use_backward": true,
    "use_judge": true,
    "use_distract": true,
    "num_paraphrase": 3,
    "device": "cuda",
    "seed": 42,
    "verbose": true,
    "save_checkpoints": true
  }