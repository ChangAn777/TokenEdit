"""
tokenedit/tokenedit_main.py
TokenEditçŸ¥è¯†ç¼–è¾‘å™¨ - å®Œå…¨ç‹¬ç«‹å®ç°
ä¸ä¾èµ–compute_ks.pyæˆ–compute_z.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple
from tqdm import tqdm

from .tokenedit_hparams import TokenEditHyperParams
from .edit_token_module import EditTokenModule
from .prompt_router import PromptRouter
from .layer_injector import LayerInjector
from .prompt_closure import PromptClosureGenerator
from .tokenedit_utils import TokenEditUtils  


class TokenEditEditor:
    """
    TokenEditçŸ¥è¯†ç¼–è¾‘å™¨
    
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. æ˜¾å¼Token (v_new, v_old)
    2. Prompté—­åŒ…è®­ç»ƒ
    3. åŠ¨æ€è·¯ç”±
    4. å±‚çº§æ³¨å…¥
    
    ä¸MEMITçš„åŒºåˆ«ï¼š
    - MEMIT: ç›´æ¥ä¿®æ”¹æƒé‡çŸ©é˜µ W
    - TokenEdit: é€šè¿‡å¯å­¦ä¹ Tokenæ³¨å…¥ h' = h + Î±*v_new + Î²*v_old
    """
    
    def __init__(self, model, tokenizer, hparams: TokenEditHyperParams):
        # ==================== [æ ¸å¿ƒä¿®å¤] å¼ºåˆ¶é”å®šéšæœºç§å­ ====================
        import random
        import numpy as np
        
        if hparams.seed is not None:
            # 1. é”å®š Python åŸç”Ÿéšæœºæ•°
            random.seed(hparams.seed)
            # 2. é”å®š Numpy éšæœºæ•°
            np.random.seed(hparams.seed)
            # 3. é”å®š PyTorch (CPU)
            torch.manual_seed(hparams.seed)
            # 4. é”å®š PyTorch (GPU)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(hparams.seed)
                # 5. å¼ºåˆ¶ CuDNN ä½¿ç”¨ç¡®å®šæ€§ç®—æ³• (è™½ç‰ºç‰²å¾®å°æ€§èƒ½ï¼Œä½†ä¿è¯å¯å¤ç°)
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False
                
            if hparams.verbose:
                print(f"ğŸ”’ å·²å¼ºåˆ¶å›ºå®šéšæœºç§å­: {hparams.seed}")
        # ===================================================================

        self.model = model
        self.tokenizer = tokenizer
        self.hparams = hparams

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(hparams.device)
        self.model.to(self.device)

        # è‡ªåŠ¨è®¾ç½®ç›®æ ‡å±‚ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if hparams.target_layers is None:
            hparams.target_layers = self._get_default_target_layers(model)
            if hparams.verbose:
                print(f"âš  æœªæŒ‡å®šç›®æ ‡å±‚ï¼Œä½¿ç”¨é»˜è®¤å€¼: {hparams.target_layers}")

        # åˆå§‹åŒ–ç»„ä»¶
        self.edit_module = None
        self.router = PromptRouter(model, tokenizer, hparams)
        self.injector = LayerInjector(hparams.target_layers)
        self.closure_gen = PromptClosureGenerator()
        self.utils = TokenEditUtils(model, tokenizer)

        # ç¼–è¾‘æ³¨å†Œè¡¨
        self.edits_registry = {}

        if hparams.verbose:
            print(f"âœ“ TokenEditEditoråˆå§‹åŒ–å®Œæˆ")
            print(f"  æ¨¡å‹: {hparams.model_name}")
            print(f"  ç›®æ ‡å±‚: {hparams.target_layers}")
            print(f"  è®¾å¤‡: {self.device}")

    def _get_default_target_layers(self, model) -> List[int]:
        """æ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ç›®æ ‡å±‚"""
        model_name = model.config._name_or_path.lower()

        # è·å–æ¨¡å‹å±‚æ•°
        if hasattr(model.config, 'n_layer'):
            num_layers = model.config.n_layer
        elif hasattr(model.config, 'num_hidden_layers'):
            num_layers = model.config.num_hidden_layers
        else:
            num_layers = 48  # GPT-2-XLçš„é»˜è®¤å±‚æ•°

        # æ ¹æ®æ¨¡å‹ç±»å‹è¿”å›ä¸åŒçš„é»˜è®¤å±‚
        if 'gpt2' in model_name or 'gpt-2' in model_name:
            if 'xl' in model_name:
                return [17, 18, 19]  # GPT-2-XLæœ‰48å±‚
            elif 'large' in model_name or 'gpt2-large' in model_name:
                return [14, 15, 16]  # GPT2-Largeæœ‰36å±‚
            elif 'medium' in model_name or 'gpt2-medium' in model_name:
                return [9, 10, 11]  # GPT2-Mediumæœ‰24å±‚
            else:
                return [5, 6, 7]  # GPT2-Smallæœ‰12å±‚
        elif 'llama' in model_name:
            # LLaMAæ¨¡å‹çš„æœ€åå‡ å±‚
            return list(range(max(0, num_layers - 3), num_layers))
        elif 'pythia' in model_name:
            return list(range(max(0, num_layers - 3), num_layers))
        else:
            # é»˜è®¤ä½¿ç”¨æœ€å3å±‚
            return list(range(max(0, num_layers - 3), num_layers))
    
    def apply_edits(self, requests: List[Dict]) -> Dict:
        """
        åº”ç”¨æ‰¹é‡ç¼–è¾‘
        
        Args:
            requests: ç¼–è¾‘è¯·æ±‚åˆ—è¡¨
                [{
                    "prompt": "The capital of France is",
                    "subject": "France",
                    "relation": "capital",
                    "target_new": "Lyon",
                    "target_true": "Paris"
                }]
        
        Returns:
            {
                "model": ç¼–è¾‘åçš„æ¨¡å‹,
                "edit_module": EditTokenæ¨¡å—,
                "stats": è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            }
        """
        num_edits = len(requests)
        
        if self.hparams.verbose:
            print(f"\n{'='*60}")
            print(f"å¼€å§‹ç¼–è¾‘ {num_edits} ä¸ªçŸ¥è¯†ç‚¹")
            print(f"{'='*60}")
        
        # 1. åˆå§‹åŒ–EditTokenæ¨¡å—
        if self.hparams.verbose:
            print("\n[1/4] åˆå§‹åŒ–EditTokenæ¨¡å—...")
        
        self.edit_module = EditTokenModule(
            hidden_size=self.model.config.hidden_size,
            num_edits=num_edits,
            hparams=self.hparams
        ).to(self.device)
        
        if self.hparams.verbose:
            print(f"  âœ“ åˆ›å»ºäº† {num_edits} å¯¹Token (v_new, v_old)")
        
        # 2. ç”ŸæˆPrompté—­åŒ…è®­ç»ƒæ•°æ®
        if self.hparams.verbose:
            print("\n[2/4] ç”ŸæˆPrompté—­åŒ…...")
        
        train_data = []
        for i, req in enumerate(requests):
            # ç”Ÿæˆå››ç±»æ ·æœ¬
            closure = self.closure_gen.generate(
                subject=req['subject'],
                relation=req.get('relation', 'capital'),  # é»˜è®¤å…³ç³»
                new_object=req['target_new'],
                old_object=req['target_true']
            )
            
            train_data.append({
                'edit_id': i,
                'closure': closure,
                'request': req
            })
            
            # æ³¨å†Œåˆ°è·¯ç”±å™¨
            self.router.register_edit(
                i, 
                req['subject'], 
                req.get('relation', 'capital')
            )
            self.edits_registry[i] = req
        
        if self.hparams.verbose:
            print(f"  âœ“ ç”Ÿæˆäº† {len(train_data)} ä¸ªPrompté—­åŒ…")
            total_samples = len(train_data) * 4  # æ¯ä¸ªé—­åŒ…4ç±»æ ·æœ¬
            print(f"  âœ“ æ€»è®­ç»ƒæ ·æœ¬æ•°: {total_samples}")
        
        # 3. è®­ç»ƒEditToken
        if self.hparams.verbose:
            print("\n[3/4] è®­ç»ƒEditToken...")
        
        stats = self._train_tokens(train_data)
        
        # 4. å®Œæˆ
        if self.hparams.verbose:
            print("\n[4/4] ç¼–è¾‘å®Œæˆ")
            print(f"  âœ“ æœ€ç»ˆæŸå¤±: {stats['losses'][-1]:.4f}")
            print(f"{'='*60}\n")
        
        return {
            'model': self.model,
            'edit_module': self.edit_module,
            'router': self.router,
            'injector': self.injector,
            'stats': stats
        }
    
    def _train_tokens(self, train_data: List[Dict]) -> Dict:
        """
        è®­ç»ƒæ˜¾å¼Token
        
        ä¼˜åŒ–ç›®æ ‡ï¼š
        L = w_edit*L_edit + w_suppress*L_suppress + 
            w_ortho*L_ortho + w_local*L_local
        """
        # å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
        for param in self.model.parameters():
            param.requires_grad = False
        
        # åªä¼˜åŒ–EditToken
        optimizer = torch.optim.Adam(
            self.edit_module.parameters(),
            lr=self.hparams.learning_rate
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.hparams.scheduler == "cosine":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.hparams.num_epochs
            )
        else:
            scheduler = None
        
        # è®­ç»ƒç»Ÿè®¡
        stats = {
            'losses': [],
            'loss_breakdown': {
                'edit': [],
                'suppress': [],
                'ortho': [],
                'local': []
            }
        }
        
        # è®­ç»ƒå¾ªç¯
        for epoch in tqdm(range(self.hparams.num_epochs), desc="Training"):
            epoch_loss = 0.0
            epoch_breakdown = {k: 0.0 for k in stats['loss_breakdown'].keys()}
            
            for data in train_data:
                edit_id = data['edit_id']
                closure = data['closure']
                
                # å¯¹æ¯ç±»æ ·æœ¬è®¡ç®—æŸå¤±
                sample_types = []
                if self.hparams.use_forward:
                    sample_types.append(('forward', closure['forward']))
                if self.hparams.use_backward:
                    sample_types.append(('backward', closure['backward']))
                if self.hparams.use_judge:
                    sample_types.append(('judge', closure['judge']))
                if self.hparams.use_distract:
                    sample_types.append(('distract', closure['distract']))
                
                for sample_type, prompts in sample_types:
                    num_samples = max(1, int(self.hparams.num_paraphrase))
                    for prompt in prompts[:num_samples]:
                        # è®¡ç®—æŸå¤±
                        losses = self._compute_sample_loss(
                            edit_id,
                            prompt,
                            sample_type,
                            closure
                        )
                        
                        # ç»¼åˆæŸå¤±
                        total_loss = (
                            self.hparams.w_edit * losses.get('edit', 0) +
                            self.hparams.w_suppress * losses.get('suppress', 0) +
                            self.hparams.w_ortho * losses.get('ortho', 0) +
                            self.hparams.w_local * losses.get('local', 0)
                        )
                        
                        # åå‘ä¼ æ’­
                        optimizer.zero_grad()
                        total_loss.backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(
                            self.edit_module.parameters(),
                            self.hparams.gradient_clip
                        )
                        
                        optimizer.step()
                        
                        # ç»Ÿè®¡
                        epoch_loss += total_loss.item()
                        for k, v in losses.items():
                            if isinstance(v, torch.Tensor):
                                epoch_breakdown[k] += v.item()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if scheduler is not None:
                scheduler.step()
            
            # è®°å½•ç»Ÿè®¡
            num_samples = len(train_data) * len(sample_types)
            stats['losses'].append(epoch_loss / num_samples)
            for k in epoch_breakdown.keys():
                stats['loss_breakdown'][k].append(
                    epoch_breakdown[k] / num_samples
                )
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0 and self.hparams.verbose:
                print(f"\nEpoch {epoch+1}/{self.hparams.num_epochs}")
                print(f"  Total Loss: {stats['losses'][-1]:.4f}")
                print(f"  Edit: {stats['loss_breakdown']['edit'][-1]:.4f}")
                print(f"  Suppress: {stats['loss_breakdown']['suppress'][-1]:.4f}")
                print(f"  Ortho: {stats['loss_breakdown']['ortho'][-1]:.4f}")
                print(f"  Local: {stats['loss_breakdown']['local'][-1]:.4f}")
        
        return stats
    
    def _compute_sample_loss(
        self,
        edit_id: int,
        prompt: str,
        sample_type: str,
        closure: Dict
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å•ä¸ªæ ·æœ¬çš„æŸå¤±
        
        Returns:
            {
                'edit': L_edit,
                'suppress': L_suppress,
                'ortho': L_ortho,
                'local': L_local
            }
        """
        losses = {}
        
        # 1. ç¼–è¾‘æˆåŠŸæŸå¤± (L_edit)
        if sample_type in ['forward', 'backward', 'judge']:
            edit_loss = self._compute_edit_loss(
                edit_id, prompt, closure['targets'][sample_type]
            )
            losses['edit'] = edit_loss
        else:
            losses['edit'] = torch.tensor(0.0, device=self.device)
        
        # 2. åäº‹å®æŠ‘åˆ¶æŸå¤± (L_suppress)
        if sample_type == 'forward':
            suppress_loss = self._compute_suppress_loss(
                edit_id, prompt, closure['old_object']
            )
            losses['suppress'] = suppress_loss
        else:
            losses['suppress'] = torch.tensor(0.0, device=self.device)
        
        # 3. æ­£äº¤æ€§æŸå¤± (L_ortho)
        # è·å–promptçš„åµŒå…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            prompt_emb = outputs.hidden_states[-1].mean(dim=1)
        
        ortho_loss = self.edit_module.compute_orthogonality_loss(prompt_emb)
        losses['ortho'] = ortho_loss
        
        # 4. å±€éƒ¨æ€§æŸå¤± (L_local)
        if sample_type == 'distract':
            local_loss = self._compute_local_loss(edit_id, prompt)
            losses['local'] = local_loss
        else:
            losses['local'] = torch.tensor(0.0, device=self.device)
        
        return losses
    
    def _compute_edit_loss(
        self,
        edit_id: int,
        prompt: str,
        target: str
    ) -> torch.Tensor:
        """
        è®¡ç®—ç¼–è¾‘æˆåŠŸæŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        ç›®æ ‡ï¼šæ¨¡å‹åº”è¯¥è¾“å‡ºtarget
        """
        # è·å–ä¸»ä½“ä½ç½® - ä½¿ç”¨å·¥å…·å‡½æ•°
        req = self.edits_registry[edit_id]
        subject_positions = self.utils.find_subject_positions(
            prompt,
            req['subject'],
            verbose=False
        )

        if not subject_positions:
            # å¦‚æœæ‰¾ä¸åˆ°ä¸»ä½“ï¼Œä½¿ç”¨ä¸€ä¸ªå°çš„é»˜è®¤æŸå¤±è€Œä¸æ˜¯0
            # è¿™æ ·å¯ä»¥è®©è®­ç»ƒç»§ç»­è¿›è¡Œ
            return torch.tensor(0.1, device=self.device)

        # ä½¿ç”¨å·¥å…·å‡½æ•°è®¡ç®—ç›®æ ‡logits
        full_text = f"{prompt} {target}"
        inputs = self.tokenizer(full_text, return_tensors="pt").to(self.device)

        # æ³¨å…¥ç¼–è¾‘å‘é‡
        self.injector.inject(
            self.model,
            edit_id,
            self.edit_module,
            subject_positions
        )

        # å‰å‘ä¼ æ’­ - åªè®¡ç®—ç›®æ ‡tokençš„æŸå¤±
        outputs = self.model(**inputs, labels=inputs['input_ids'])

        # æ¸…é™¤æ³¨å…¥
        self.injector.clear()

        return outputs.loss
    
    def _compute_suppress_loss(
        self,
        edit_id: int,
        prompt: str,
        old_target: str
    ) -> torch.Tensor:
        """
        è®¡ç®—åäº‹å®æŠ‘åˆ¶æŸå¤±ï¼ˆUnlikelihood Lossï¼‰
        ç›®æ ‡ï¼šé™ä½æ—§ç­”æ¡ˆçš„æ¦‚ç‡
        
        L_suppress = -log(1 - P(old_target | prompt))
        """
        # ç¼–ç 
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        old_tokens = self.tokenizer.encode(old_target, add_special_tokens=False)
        
        if len(old_tokens) == 0:
            return torch.tensor(0.0, device=self.device)
        
        # è·å–ä¸»ä½“ä½ç½®
        req = self.edits_registry[edit_id]
        subject = req['subject']
        subject_tokens = self.tokenizer.encode(subject, add_special_tokens=False)
        subject_positions = list(range(1, 1 + len(subject_tokens)))
        
        # æ³¨å…¥
        self.injector.inject(
            self.model,
            edit_id,
            self.edit_module,
            subject_positions
        )
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits[0, -1, :]  # æœ€åä¸€ä¸ªtokençš„logits
            probs = F.softmax(logits, dim=-1)
        
        # æ¸…é™¤æ³¨å…¥
        self.injector.clear()
        
        # è®¡ç®—æ—§tokençš„æ¦‚ç‡
        old_token_id = old_tokens[0]
        old_prob = probs[old_token_id]
        
        # Unlikelihood loss
        loss = -torch.log(1 - old_prob + 1e-8)
        
        return loss
    
    def _compute_local_loss(
        self,
        edit_id: int,
        prompt: str
    ) -> torch.Tensor:
        """
        è®¡ç®—å±€éƒ¨æ€§æŸå¤±ï¼ˆKLæ•£åº¦ï¼‰
        ç›®æ ‡ï¼šå¹²æ‰°é—®é¢˜çš„è¾“å‡ºåº”è¯¥ä¸åŸæ¨¡å‹ä¸€è‡´
        """
        # è·å–ä¸»ä½“ä½ç½®  
        req = self.edits_registry[edit_id]
        subject_positions = self.utils.find_subject_positions(
            prompt,
            req['subject'],
            verbose=False
        )
        
        if not subject_positions:
            return torch.tensor(0.0, device=self.device)
        
        # ä½¿ç”¨å·¥å…·å‡½æ•°è®¡ç®—KLæ•£åº¦  
        kl_loss = self.utils.compute_kl_divergence(
            prompt,
            subject_positions,
            self.edit_module,
            edit_id,
            self.injector
        )
        
        return kl_loss
    
    def inference(self, prompt: str, max_new_tokens: int = 10, verbose: bool = None) -> str:
        """
        æ¨ç†ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ³¨å…¥ç¼–è¾‘

        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆNoneåˆ™ä½¿ç”¨hparams.verboseï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if verbose is None:
            verbose = self.hparams.verbose

        self.model.eval()

        # 1. ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # 2. è·¯ç”±æ£€æµ‹
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            prompt_emb = outputs.hidden_states[-1].mean(dim=1)

        edit_id = self.router.route(prompt, prompt_emb)

        # 3. æ¡ä»¶åŒ–æ³¨å…¥
        if edit_id is not None:
            if verbose:
                req = self.edits_registry[edit_id]
                print(f"âœ“ è§¦å‘ç¼–è¾‘ #{edit_id}: {req['subject']} -> {req['target_new']}")

            # è·å–ä¸»ä½“ä½ç½® - ä½¿ç”¨å·¥å…·å‡½æ•°
            req = self.edits_registry[edit_id]
            subject_positions = self.utils.find_subject_positions(
                prompt,
                req['subject'],
                verbose=verbose
            )

            if subject_positions:
                # æ³¨å…¥
                self.injector.inject(
                    self.model,
                    edit_id,
                    self.edit_module,
                    subject_positions
                )
                if verbose:
                    print(f"  æ³¨å…¥ä½ç½®: {subject_positions}")
            else:
                if verbose:
                    print(f"  è­¦å‘Š: æœªæ‰¾åˆ°ä¸»ä½“ä½ç½®ï¼Œç¼–è¾‘å¯èƒ½æ— æ•ˆ")
        else:
            if verbose:
                print("âœ— æœªè§¦å‘ç¼–è¾‘ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")

        # 4. ç”Ÿæˆ
        with torch.no_grad():
            output_ids = self.model.generate(
                inputs['input_ids'],
                attention_mask=inputs.get('attention_mask'),
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # æ¸…é™¤æ³¨å…¥
        self.injector.clear()

        # è§£ç 
        result = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)

        return result
    
    def save(self, path: str):
        """ä¿å­˜ç¼–è¾‘å™¨çŠ¶æ€"""
        torch.save({
            'edit_module': self.edit_module.state_dict(),
            'edits_registry': self.edits_registry,
            'hparams': self.hparams
        }, path)
        if self.hparams.verbose:
            print(f"âœ“ ç¼–è¾‘å™¨å·²ä¿å­˜åˆ° {path}")
    
    def load(self, path: str):
        """åŠ è½½ç¼–è¾‘å™¨çŠ¶æ€"""
        checkpoint = torch.load(path)
        
        # æ¢å¤EditModule
        num_edits = len(checkpoint['edits_registry'])
        self.edit_module = EditTokenModule(
            self.model.config.hidden_size,
            num_edits,
            self.hparams
        ).to(self.device)
        self.edit_module.load_state_dict(checkpoint['edit_module'])
        
        # æ¢å¤æ³¨å†Œè¡¨
        self.edits_registry = checkpoint['edits_registry']
        
        if self.hparams.verbose:
            print(f"âœ“ ç¼–è¾‘å™¨å·²ä» {path} åŠ è½½")
            print(f"  åŒ…å« {num_edits} ä¸ªç¼–è¾‘")
